{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6f7db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ead1735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining expansion\n",
    "\n",
    "text = 'text=\"She is not is contracted to she isn’t or she’s not. I am not is only contracted to I’m not.\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dad4a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove contractions from text\n",
    "# install contractions\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "953df4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text=\"She is not is contracted to she is not or she is not. I am not is only contracted to I am not.\"'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e81e35bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789\n",
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
      "0123456789abcdefABCDEF\n",
      "01234567\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
      "\u000b\f\n",
      " \t\n",
      "\u000b\f\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.digits)\n",
    "print(string.ascii_letters)\n",
    "print(string.ascii_lowercase)\n",
    "print(string.ascii_uppercase)\n",
    "print(string.hexdigits)\n",
    "print(string.octdigits)\n",
    "print(string.punctuation)\n",
    "print(string.printable)\n",
    "print(string.whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "888ad656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(input_text):\n",
    "    for i in string.digits:\n",
    "        input_text = input_text.replace(i,'')\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "27e2d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James@Bond, Mercedez@Benzq, LexusV\n"
     ]
    }
   ],
   "source": [
    "text = 'James@Bond007, Mercedez@Benzq, LexusV4'\n",
    "\n",
    "text = remove_digits(text)\n",
    "print(text)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2ed93ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(input_text):\n",
    "    for i in string.punctuation:\n",
    "        input_text = input_text.replace(i,'')\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c5548f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JamesBond MercedezBenzq LexusV'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_punctuation(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25488253",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "043ba29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0b1ffe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a7a58a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Anime is a diverse medium with distinctive production methods that have adapted in response to emergent technologies. It combines graphic art, characterization, cinematography, and other forms of imaginative and individualistic techniques.[3] Compared to Western animation, anime production generally focuses less on movement, and more on the detail of settings and use of \"camera effects\", such as panning, zooming, and angle shots.[3] Diverse art styles are used, and character proportions and features can be quite varied, with a common characteristic feature being large and emotive eyes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "43564edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anime is a diverse medium with distinctive production methods that have adapted in response to emergent technologies.', 'It combines graphic art, characterization, cinematography, and other forms of imaginative and individualistic techniques.', '[3] Compared to Western animation, anime production generally focuses less on movement, and more on the detail of settings and use of \"camera effects\", such as panning, zooming, and angle shots.', '[3] Diverse art styles are used, and character proportions and features can be quite varied, with a common characteristic feature being large and emotive eyes']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenizer\n",
    "\n",
    "text_sent = sent_tokenize(text)\n",
    "print(text_sent)\n",
    "print(len(text_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "822c425c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anime', 'is', 'a', 'diverse', 'medium', 'with', 'distinctive', 'production', 'methods', 'that', 'have', 'adapted', 'in', 'response', 'to', 'emergent', 'technologies', '.', 'It', 'combines', 'graphic', 'art', ',', 'characterization', ',', 'cinematography', ',', 'and', 'other', 'forms', 'of', 'imaginative', 'and', 'individualistic', 'techniques', '.', '[', '3', ']', 'Compared', 'to', 'Western', 'animation', ',', 'anime', 'production', 'generally', 'focuses', 'less', 'on', 'movement', ',', 'and', 'more', 'on', 'the', 'detail', 'of', 'settings', 'and', 'use', 'of', '``', 'camera', 'effects', \"''\", ',', 'such', 'as', 'panning', ',', 'zooming', ',', 'and', 'angle', 'shots', '.', '[', '3', ']', 'Diverse', 'art', 'styles', 'are', 'used', ',', 'and', 'character', 'proportions', 'and', 'features', 'can', 'be', 'quite', 'varied', ',', 'with', 'a', 'common', 'characteristic', 'feature', 'being', 'large', 'and', 'emotive', 'eyes']\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "text_words = word_tokenize(text)\n",
    "print(text_words)\n",
    "print(len(text_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d1139d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "04ad0348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    for i in stopwords.words('english'):\n",
    "        input_text = input_text.replace(i,'')\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9e7e046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ane   re eu wh nc prucn eh h h pe n pne  eergen echlge. I cbne grphc r, chrcerzn, cnegrph, n r  f gn n nvulc echnque.[3] Cp  Weern nn, ne prucn gener fcue le n en, n  n  el f eng n ue f \"cer effec\", uch  pnnng, zng, n ngle h.[3] Dre r le  ue, n chrcer prprn n feu cn  que v, wh  cn chrcerc feu ng lrge n e ee\n"
     ]
    }
   ],
   "source": [
    "text = remove_stopwords(text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ef9a8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ane', 're', 'eu', 'wh', 'nc', 'prucn', 'eh', 'h', 'h', 'pe', 'n', 'pne', 'eergen', 'echlge.', 'I', 'cbne', 'grphc', 'r,', 'chrcerzn,', 'cnegrph,', 'n', 'r', 'f', 'gn', 'n', 'nvulc', 'echnque.[3]', 'Cp', 'Weern', 'nn,', 'ne', 'prucn', 'gener', 'fcue', 'le', 'n', 'en,', 'n', 'n', 'el', 'f', 'eng', 'n', 'ue', 'f', '\"cer', 'effec\",', 'uch', 'pnnng,', 'zng,', 'n', 'ngle', 'h.[3]', 'Dre', 'r', 'le', 'ue,', 'n', 'chrcer', 'prprn', 'n', 'feu', 'cn', 'que', 'v,', 'wh', 'cn', 'chrcerc', 'feu', 'ng', 'lrge', 'n', 'e', 'ee']\n"
     ]
    }
   ],
   "source": [
    "# Lets split the text\n",
    "text = text.split()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ac88e9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ane', 'eu', 'wh', 'nc', 'prucn', 'eh', 'h', 'h', 'pe', 'n', 'pne', 'eergen', 'echlge.', 'I', 'cbne', 'grphc', 'r,', 'chrcerzn,', 'cnegrph,', 'n', 'r', 'f', 'gn', 'n', 'nvulc', 'echnque.[3]', 'Cp', 'Weern', 'nn,', 'ne', 'prucn', 'gener', 'fcue', 'le', 'n', 'en,', 'n', 'n', 'el', 'f', 'eng', 'n', 'ue', 'f', '\"cer', 'effec\",', 'uch', 'pnnng,', 'zng,', 'n', 'ngle', 'h.[3]', 'Dre', 'r', 'le', 'ue,', 'n', 'chrcer', 'prprn', 'n', 'feu', 'cn', 'que', 'v,', 'wh', 'cn', 'chrcerc', 'feu', 'ng', 'lrge', 'n', 'e', 'ee']\n"
     ]
    }
   ],
   "source": [
    "# now we remove the stopwords \n",
    "\n",
    "for j in stopwords.words('english'):\n",
    "    # check if the stopword is present before attempting removal\n",
    "    if j in text:\n",
    "        text.remove(j)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34037d6",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "abf171be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'happili', 'bigger', 'connect', 'connect']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# initialise the inbuilt stemmer\n",
    "words = [\"running\", \"flies\", \"happily\", \"bigger\", \"connected\", \"connection\"]\n",
    "stemmer = PorterStemmer()\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in words]\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4fdec5",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "952d1fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\chara\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloading wordnet before applying lemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d531f605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'happily', 'bigger', 'connected', 'connection']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## we can also use Lemmatizer instead of Stemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2755b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea56a71",
   "metadata": {},
   "source": [
    "# putting all the steps together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fb4ebe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we are Mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0    We are Learning Machine Learning $\n",
       "1   Processing natural - language data.\n",
       "2      10 machine - learning algorithms\n",
       "3  we are Mimicing natural intelligence"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_text = ['We are Learning Machine Learning $',\n",
    "            \"Processing natural - language data.\",\n",
    "            '10 machine - learning algorithms',\n",
    "            'we are Mimicing natural intelligence']\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "188defd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean(doc):\n",
    "    # doc is a string of text\n",
    "    # let's define a regex to match special characters and digits\n",
    "    regex = '[^a-zA-Z.]'\n",
    "    doc = re.sub(regex, ' ', doc)\n",
    "    # convert to lowercase\n",
    "    doc = doc.lower()\n",
    "    # tokenization\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    # Stop word removal \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # join and return \n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b500f380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "      <td>learning machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "      <td>processing natural language data .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms</td>\n",
       "      <td>machine learning algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we are Mimicing natural intelligence</td>\n",
       "      <td>mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text                          clean_text\n",
       "0    We are Learning Machine Learning $           learning machine learning\n",
       "1   Processing natural - language data.  processing natural language data .\n",
       "2      10 machine - learning algorithms          machine learning algorithm\n",
       "3  we are Mimicing natural intelligence       mimicing natural intelligence"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_text'] = df['text'].apply(lambda x : clean(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca4a63",
   "metadata": {},
   "source": [
    "# Count Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3def0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the text_dtm (# of docs, # of unique vocabulary): (4, 9)\n",
      "Vocab: ['algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# import feature extraction methods from sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# instantiate a vectorizer\n",
    "bow_vect = CountVectorizer()\n",
    "\n",
    "# use it to extract features from training data \n",
    "text_dtm = bow_vect.fit_transform(df['clean_text'])\n",
    "\n",
    "print(f'Shape of the text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}')\n",
    "print(f'Vocab: {bow_vect.get_feature_names_out()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a47e9fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm  data  intelligence  language  learning  machine  mimicing  \\\n",
       "0          0     0             0         0         2        1         0   \n",
       "1          0     1             0         1         0        0         0   \n",
       "2          1     0             0         0         1        1         0   \n",
       "3          0     0             1         0         0        0         1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_dtm.toarray(), columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9013c",
   "metadata": {},
   "source": [
    "# Binary Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "1e63e8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the text_dtm (# of docs, # of unique vocabulary): (4, 9)\n",
      "Vocab: ['algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n"
     ]
    }
   ],
   "source": [
    "# import feature extraction methods from sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# instantiate a vectorizer\n",
    "bow_vect = CountVectorizer(binary=True)\n",
    "\n",
    "# use it to extract features from training data \n",
    "text_dtm = bow_vect.fit_transform(df['clean_text'])\n",
    "\n",
    "print(f'Shape of the text_dtm (# of docs, # of unique vocabulary): {text_dtm.shape}')\n",
    "print(f'Vocab: {bow_vect.get_feature_names_out()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "579fd96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm  data  intelligence  language  learning  machine  mimicing  \\\n",
       "0          0     0             0         0         1        1         0   \n",
       "1          0     1             0         1         0        0         0   \n",
       "2          1     0             0         0         1        1         0   \n",
       "3          0     0             1         0         0        0         1   \n",
       "\n",
       "   natural  processing  \n",
       "0        0           0  \n",
       "1        1           1  \n",
       "2        0           0  \n",
       "3        1           0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(text_dtm.toarray(),columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49ccf15",
   "metadata": {},
   "source": [
    "# TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b24a88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['algorithm' 'data' 'intelligence' 'language' 'learning' 'machine'\n",
      " 'mimicing' 'natural' 'processing']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.         0.         0.89442719 0.4472136\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.52547275 0.         0.52547275 0.         0.\n",
      "  0.         0.41428875 0.52547275]\n",
      " [0.66767854 0.         0.         0.         0.52640543 0.52640543\n",
      "  0.         0.         0.        ]\n",
      " [0.         0.         0.61761437 0.         0.         0.\n",
      "  0.61761437 0.48693426 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Initialize Tf-Idf Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform text into Tf-Idf representation\n",
    "tfidf_matrix = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Display results\n",
    "print('Vocabulary: ',vectorizer.get_feature_names_out())\n",
    "print('TF-IDF Matrix:\\n', tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e60dd8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>algorithm</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>mimicing</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.525473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.667679</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.526405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617614</td>\n",
       "      <td>0.486934</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   algorithm      data  intelligence  language  learning   machine  mimicing  \\\n",
       "0   0.000000  0.000000      0.000000  0.000000  0.894427  0.447214  0.000000   \n",
       "1   0.000000  0.525473      0.000000  0.525473  0.000000  0.000000  0.000000   \n",
       "2   0.667679  0.000000      0.000000  0.000000  0.526405  0.526405  0.000000   \n",
       "3   0.000000  0.000000      0.617614  0.000000  0.000000  0.000000  0.617614   \n",
       "\n",
       "    natural  processing  \n",
       "0  0.000000    0.000000  \n",
       "1  0.414289    0.525473  \n",
       "2  0.000000    0.000000  \n",
       "3  0.486934    0.000000  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidf_matrix.toarray(),columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c30548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
